Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Behnel2011,
abstract = {Cython is a Python language extension that allows explicit type declarations and is compiled directly to C. As such, it addresses Python's large overhead for numerical loops and the difficulty of efficiently using existing C and Fortran code, which Cython can interact with natively.},
author = {Behnel, Stefan and Bradshaw, Robert and Citro, Craig and Dalcin, Lisandro and Seljebotn, Dag Sverre and Smith, Kurt},
doi = {10.1109/MCSE.2010.118},
issn = {1521-9615},
journal = {Comput. Sci. Eng.},
keywords = {Cython,Python,numerics,scientific computing},
month = {mar},
number = {2},
pages = {31--39},
publisher = {IEEE Computer Society},
title = {{Cython: The Best of Both Worlds}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5582062},
volume = {13},
year = {2011}
}
@article{Teyssier2001,
abstract = {A new N-body and hydrodynamical code, called RAMSES, is presented. It has been designed to study structure formation in the universe with high spatial resolution. The code is based on Adaptive Mesh Refinement (AMR) technique, with a tree based data structure allowing recursive grid refinements on a cell-by-cell basis. The N-body solver is very similar to the one developed for the ART code (Kravtsov et al. 97), with minor differences in the exact implementation. The hydrodynamical solver is based on a second-order Godunov method, a modern shock-capturing scheme known to compute accurately the thermal history of the fluid component. The accuracy of the code is carefully estimated using various test cases, from pure gas dynamical tests to cosmological ones. The specific refinement strategy used in cosmological simulations is described, and potential spurious effects associated to shock waves propagation in the resulting AMR grid are discussed and found to be negligible. Results obtained in a large N-body and hydrodynamical simulation of structure formation in a low density LCDM universe are finally reported, with 256{\^{}}3 particles and 4.1 10{\^{}}7 cells in the AMR grid, reaching a formal resolution of 8192{\^{}}3. A convergence analysis of different quantities, such as dark matter density power spectrum, gas pressure power spectrum and individual haloes temperature profiles, shows that numerical results are converging down to the actual resolution limit of the code, and are well reproduced by recent analytical predictions in the framework of the halo model.},
archivePrefix = {arXiv},
arxivId = {astro-ph/0111367},
author = {Teyssier, Romain},
doi = {10.1051/0004-6361:20011817},
eprint = {0111367},
issn = {0004-6361},
journal = {Astron. Astrophys. v.385, p.337-364},
keywords = {COSMOLOGY: LARGE-SCALE STRUCTURE OF UNIVERSE,COSMOLOGY: THEORY,GRAVITATION,HYDRODYNAMICS,METHODS: NUMERICAL},
month = {nov},
pages = {337--364},
primaryClass = {astro-ph},
title = {{Cosmological Hydrodynamics with Adaptive Mesh Refinement: a new high resolution code called RAMSES}},
url = {http://arxiv.org/abs/astro-ph/0111367 http://dx.doi.org/10.1051/0004-6361:20011817},
volume = {385},
year = {2001}
}
@article{Juric2015,
abstract = {The Large Synoptic Survey Telescope (LSST; Ivezic et al. 2008, http://lsst.org) is a planned, large-aperture, wide-field, ground-based telescope that will survey half the sky every few nights in six optical bands from 320 to 1050 nm. It will explore a wide range of astrophysical questions, ranging from discovering killer asteroids, to examining the nature of dark energy. LSST will produce on average 15 terabytes of data per night, yielding an (uncompressed) data set of 200 petabytes at the end of its 10-year mission. Dedicated HPC facilities (with a total of 320 TFLOPS at start, scaling up to 1.7 PFLOPS by the end) will process the image data in near real time, with full-dataset reprocessing on annual scale. The nature, quality, and volume of LSST data will be unprecedented, so the data system design requires petascale storage, terascale computing, and gigascale communications.},
author = {Juric, Mario and Tyson, Tony},
doi = {10.1017/S174392131401285X},
issn = {1743-9213},
journal = {Proc. Int. Astron. Union},
month = {mar},
number = {H16},
pages = {675--676},
title = {{LSST Data Management: Entering the Era of Petascale Optical Astronomy}},
url = {http://adsabs.harvard.edu/abs/2015HiA....16..675J},
volume = {10},
year = {2015}
}
@article{Sinha2007,
author = {Sinha, Rishi Rakesh and Winslett, Marianne},
doi = {10.1145/1272743.1272746},
issn = {03625915},
journal = {ACM Trans. Database Syst.},
month = {aug},
number = {3},
pages = {16--es},
title = {{Multi-resolution bitmap indexes for scientific data}},
url = {http://portal.acm.org/citation.cfm?doid=1272743.1272746},
volume = {32},
year = {2007}
}
@inbook{Hilbert1970,
address = {Berlin, Heidelberg},
author = {Hilbert, David},
chapter = {{\{}{\"{U}}{\}}ber die},
doi = {10.1007/978-3-662-25726-5_1},
isbn = {978-3-662-25726-5},
pages = {1--2},
publisher = {Springer Berlin Heidelberg},
title = {{Gesammelte Abhandlungen: Band III: Analysis {\{}$\backslash$textperiodcentered{\}} Grundlagen der Mathematik Physik {\{}$\backslash$textperiodcentered{\}} Verschiedenes Lebensgeschichte}},
url = {http://dx.doi.org/10.1007/978-3-662-25726-5{\_}1},
year = {1970}
}
@article{Connor2010,
abstract = {We present a parallel algorithm for k-nearest neighbor graph construction that uses Morton ordering. Experiments show that our approach has the following advantages over existing methods: 1) faster construction of k-nearest neighbor graphs in practice on multicore machines, 2) less space usage, 3) better cache efficiency, 4) ability to handle large data sets, and 5) ease of parallelization and implementation. If the point set has a bounded expansion constant, our algorithm requires one-comparison-based parallel sort of points, according to Morton order plus near-linear additional steps to output the k-nearest neighbor graph.},
author = {Connor, Michael and Kumar, Piyush},
doi = {10.1109/TVCG.2010.9},
issn = {1077-2626},
journal = {IEEE Trans. Vis. Comput. Graph.},
keywords = {Algorithms,Computer Graphics,Computer Simulation,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Models, Theoretical,User-Computer Interface},
month = {jan},
number = {4},
pages = {599--608},
pmid = {20467058},
shorttitle = {IEEE Transactions on Visualization and Computer Gr},
title = {{Fast construction of k-nearest neighbor graphs for point clouds.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20467058},
volume = {16},
year = {2010}
}
@incollection{ONeil1989,
address = {Berlin, Heidelberg},
author = {O'Neil, Patrick E.},
booktitle = {High Perform. Trans. Syst. 2nd Int. Work. Asilomar Conf. Center, Pacific Grove, CA, USA Sept. 28--30, 1987 Proc.},
chapter = {MODEL 204},
doi = {10.1007/3-540-51085-0},
editor = {Gawlick, Dieter and Haynie, Mark and Reuter, Andreas},
isbn = {978-3-540-51085-7},
pages = {39--59},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{High Performance Transaction Systems}},
url = {http://www.springerlink.com/index/10.1007/3-540-51085-0},
volume = {359},
year = {1989}
}
@article{Kaser2016,
author = {Kaser, Owen and Lemire, Daniel},
doi = {10.1002/spe.2289},
issn = {00380644},
journal = {Softw. Pract. Exp.},
month = {feb},
number = {2},
pages = {167--198},
title = {{Compressed bitmap indexes: beyond unions and intersections}},
url = {http://doi.wiley.com/10.1002/spe.2289},
volume = {46},
year = {2016}
}
@article{Malensek2014,
author = {Malensek, Matthew and Pallickara, Sangmi and Pallickara, Shrideep},
doi = {10.1109/MCSE.2014.48},
issn = {1521-9615},
journal = {Comput. Sci. Eng.},
month = {jul},
number = {4},
pages = {53--61},
title = {{Evaluating Geospatial Geometry and Proximity Queries Using Distributed Hash Tables}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6785924},
volume = {16},
year = {2014}
}
@inproceedings{Sinha2006,
author = {Sinha, R.R. and Mitra, S. and Winslett, M.},
booktitle = {Proc. 20th IEEE Int. Parallel Distrib. Process. Symp.},
doi = {10.1109/IPDPS.2006.1639304},
isbn = {1-4244-0054-6},
pages = {10 pp.},
publisher = {IEEE},
title = {{Bitmap indexes for large scientific data sets: a case study}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1639304},
year = {2006}
}
@article{Springel2005b,
abstract = {We discuss the cosmological simulation code GADGET-2, a new massively parallel TreeSPH code, capable of following a collisionless fluid with the N-body method, and an ideal gas by means of smoothed particle hydrodynamics (SPH). Our implementation of SPH manifestly conserves energy and entropy in regions free of dissipation, while allowing for fully adaptive smoothing lengths. Gravitational forces are computed with a hierarchical multipole expansion, which can optionally be applied in the form of a TreePM algorithm, where only short-range forces are computed with the `tree' method while long-range forces are determined with Fourier techniques. Time integration is based on a quasi-symplectic scheme where long-range and short-range forces can be integrated with different time-steps. Individual and adaptive short-range time-steps may also be employed. The domain decomposition used in the parallelization algorithm is based on a space-filling curve, resulting in high flexibility and tree force errors that do not depend on the way the domains are cut. The code is efficient in terms of memory consumption and required communication bandwidth. It has been used to compute the first cosmological N-body simulation with more than 1010 dark matter particles, reaching a homogeneous spatial dynamic range of 105 per dimension in a three-dimensional box. It has also been used to carry out very large cosmological SPH simulations that account for radiative cooling and star formation, reaching total particle numbers of more than 250 million. We present the algorithms used by the code and discuss their accuracy and performance using a number of test problems. GADGET-2 is publicly released to the research community.},
author = {Springel, Volker},
doi = {10.1111/j.1365-2966.2005.09655.x},
issn = {0035-8711},
journal = {Mon. Not. R. Astron. Soc.},
keywords = {codes: GADGET,dark matter,galaxies: interactions,methods: numerical},
mendeley-tags = {codes: GADGET},
month = {dec},
number = {4},
pages = {1105--1134},
title = {{The cosmological simulation code gadget-2}},
url = {http://adsabs.harvard.edu/abs/2005MNRAS.364.1105S},
volume = {364},
year = {2005}
}
@inproceedings{Shoshani1999,
abstract = {In many scientific domains, experimental devices or simulation programs generate large volumes of data. The volumes of data may reach hundreds of terabytes and therefore it is impractical to store them on disk systems. Rather they are stored on robotic tape systems that are managed by some mass storage system (MSS). A major bottleneck in analyzing the simulated/collected data is the retrieval of subsets from the tertiary storage system. We describe the architecture and implementation of a Storage Access Coordination System (STACS) designed to optimize the use of a disk cache, and thus minimize the number of files read from tape. We achieve this by using a specialized index to locate the relevant data on tapes, and by coordinating file caching over multiple queries. We focus on a specific application area, a high energy physics data management and analysis environment. STACS was implemented and is being incorporated in an operational system, scheduled to go online at the end of 1999. We also include the results of various tests that demonstrate the benefits and efficiency gained of using the STACS},
author = {Shoshani, A. and Bernardo, L.M. and Nordberg, H. and Rotem, D. and Sim, A.},
booktitle = {Proceedings. Elev. Int. Conf. Sci. Stat. Database Manag.},
doi = {10.1109/SSDM.1999.787637},
isbn = {0-7695-0046-3},
keywords = {Analytical models,Cache storage,Data analysis,Design optimization,Energy management,Environmental management,Indexing,Information retrieval,Multidimensional systems,Robot kinematics,STACS,Storage Access Coordination System,cache storage,database indexing,disk cache,disk systems,experimental devices,file caching,high energy physics data management,mass storage system,multidimensional indexing,multiple queries,operational system,physics computing,query coordination,query processing,robotic tape systems,scientific domains,scientific information systems,simulation programs,specialized index,storage management,subset retrieval,tertiary storage management,tertiary storage system},
pages = {214--225},
publisher = {IEEE Comput. Soc},
shorttitle = {Scientific and Statistical Database Management, 19},
title = {{Multidimensional indexing and query coordination for tertiary storage management}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=787637},
year = {1999}
}
@article{Croft2015,
abstract = {Simulations of the entire universe are by definition models of the largest and most complex physical system that exists. In carrying them out, simulators seek to discretize the matter (and radiation) in a model universe and follow their evolution from the Big Bang to the present day.},
author = {Croft, Rupert and {Di Matteo}, Tiziana and Khandai, Nishikanta},
doi = {10.1109/MCSE.2015.5},
issn = {1521-9615},
journal = {Comput. Sci. Eng.},
keywords = {Big Bang,Computational modeling,Cosmological Simulations,Gravity,HPC,High performance computing,Hydrodynamics,Instruction sets,Message systems,Scientific computing,Solid modeling,Universe,complex physical system,cosmology,high-performance computing,model universe,petascale cosmology,scientific computing,structure formation,supercomputer},
mendeley-tags = {Cosmological Simulations},
month = {mar},
number = {2},
pages = {40--46},
shorttitle = {Computing in Science {\&} Engineering},
title = {{Petascale Cosmology: Simulations of Structure Formation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7006381},
volume = {17},
year = {2015}
}
@article{Lemire2010,
author = {Lemire, Daniel and Kaser, Owen and Aouiche, Kamel},
doi = {10.1016/j.datak.2009.08.006},
issn = {0169023X},
journal = {Data Knowl. Eng.},
keywords = {Compression,EWAH,Gray codes,Indexing,Multidimensional databases},
mendeley-tags = {EWAH},
month = {jan},
number = {1},
pages = {3--28},
publisher = {Elsevier Science Publishers B. V.},
title = {{Sorting improves word-aligned bitmap indexes}},
url = {http://dl.acm.org/citation.cfm?id=1663645.1663682},
volume = {69},
year = {2010}
}
@article{Juric2013,
abstract = {The Large Synoptic Survey Telescope (LSST; http://lsst.org) is a planned, large-aperture, wide-field, ground-based telescope that will survey half the sky every few nights in six optical bands from 320 to 1050 nm. It will explore a wide range of astrophysical questions, from studies of the Solar System, to examining the nature of dark energy. LSST is an integrated survey system. The observatory, telescope, camera and data management systems will be built to conduct the LSST survey and will not support 'PI mode' in the classical sense. Instead, the ultimate, science-enabling, deliverable of LSST will be the fully reduced data. This poster presents the baseline design and contents of LSST data products. There will be three main categories. "Level 1" data products will be generated continuously every observing night and include measurements such as alerts to objects that have changed brightness or position. They will be broadcast world-wide using VO protocols. "Level 2" data products will be made available as annual Data Releases and will include images and measurements of quantities such as positions, fluxes, and shapes, as well as variability information such as orbital parameters for moving objects and an appropriate compact description of light curves. The exact contents of Level 2 products will be set by the desire to minimize the necessity to independently reprocess the image data. Finally, approximately 10{\%} of LSST's computing capability will be made available to the community for generation of "Level 3" data products. These will be used to perform custom analyses not fully enabled by Level 1/2, while taking advantage of co-location of computation with the entire LSST data set.},
author = {Juric,  Mario and Kantor,  J. and Axelrod,  T. S. and Dubois-Felsmann,  G. P. and Becla,  J. and Lim,  K. and {LSST Collaboration} and {LSST Science Collaborations}},
journal = {Am. Astron. Soc.},
title = {{LSST Data Products: Enabling LSST Science}},
url = {http://adsabs.harvard.edu/abs/2013AAS...22124701J},
year = {2013}
}
@inproceedings{Wu1998,
author = {Wu, Ming-Chuan and Buchmann, A.P.},
booktitle = {Proc. 14th Int. Conf. Data Eng.},
doi = {10.1109/ICDE.1998.655780},
isbn = {0-8186-8289-2},
pages = {220--230},
publisher = {IEEE Comput. Soc},
title = {{Encoded bitmap indexing for data warehouses}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=655780},
year = {1998}
}
@article{Turk20d11a,
author = {Turk, Matthew J. and Smith, Britton D. and Oishi, Jeffrey S. and Skory, Stephen and Skillman, Samuel W. and Abel, Tom and Norman, Michael L.},
doi = {10.1088/0067-0049/192/1/9},
issn = {0067-0049},
journal = {Astrophys. J. Suppl. Ser.},
month = {jan},
number = {1},
pages = {9},
title = {{yt: A MULTI-CODE ANALYSIS TOOLKIT FOR ASTROPHYSICAL SIMULATION DATA}},
url = {http://stacks.iop.org/0067-0049/192/i=1/a=9?key=crossref.ad58f4fc8c6272d93975507bf0238006},
volume = {192},
year = {2011}
}
@inproceedings{Stockinger2004,
address = {Berlin, Heidelberg},
author = {Stockinger, Kurt and Wu, Kesheng and Shoshani, Arie},
booktitle = {Database Expert Syst. Appl. 15th Int. Conf. DEXA 2004, Zaragoza, Spain, August 30-September 3, 2004. Proc.},
chapter = {Evaluation},
doi = {10.1007/978-3-540-30075-5_12},
editor = {Galindo, Fernando and Takizawa, Makoto and Traunm{\"{u}}ller, Roland},
isbn = {978-3-540-30075-5},
pages = {120--129},
publisher = {Springer Berlin Heidelberg},
title = {{Evaluation Strategies for Bitmap Indices with Binning}},
url = {http://dx.doi.org/10.1007/978-3-540-30075-5{\_}12},
year = {2004}
}
@article{Wu2001,
author = {Wu, Kesheng and Otoo, Ekow and Shoshani, Arie},
journal = {Lawrence Berkeley Natl. Lab.},
keywords = {EWAH,database index compression bitmap},
mendeley-tags = {EWAH},
month = {sep},
title = {{Compressed bitmap indices for efficient query processing}},
url = {http://escholarship.org/uc/item/8k22w7q2},
year = {2001}
}
@article{Hopkins2015,
abstract = {We present two new Lagrangian methods for hydrodynamics, in a systematic comparison with moving-mesh, smoothed particle hydrodynamics (SPH), and stationary (non-moving) grid methods. The new methods are designed to simultaneously capture advantages of both SPH and grid-based/adaptive mesh refinement (AMR) schemes. They are based on a kernel discretization of the volume coupled to a high-order matrix gradient estimator and a Riemann solver acting over the volume `overlap'. We implement and test a parallel, second-order version of the method with self-gravity and cosmological integration, in the code GIZMO:1 this maintains exact mass, energy and momentum conservation; exhibits superior angular momentum conservation compared to all other methods we study; does not require `artificial diffusion' terms; and allows the fluid elements to move with the flow, so resolution is automatically adaptive. We consider a large suite of test problems, and find that on all problems the new methods appear competitive with moving-mesh schemes, with some advantages (particularly in angular momentum conservation), at the cost of enhanced noise. The new methods have many advantages versus SPH: proper convergence, good capturing of fluid-mixing instabilities, dramatically reduced `particle noise' and numerical viscosity, more accurate sub-sonic flow evolution, and sharp shock-capturing. Advantages versus non-moving meshes include: automatic adaptivity, dramatically reduced advection errors and numerical overmixing, velocity-independent errors, accurate coupling to gravity, good angular momentum conservation and elimination of `grid alignment' effects. We can, for example, follow hundreds of orbits of gaseous discs, while AMR and SPH methods break down in a few orbits. However, fixed meshes minimize `grid noise'. These differences are important for a range of astrophysical problems.},
author = {Hopkins, P. F.},
doi = {10.1093/mnras/stv195},
issn = {0035-8711},
journal = {Mon. Not. R. Astron. Soc.},
keywords = {cosmology: theory,hydrodynamics,instabilities,methods: numerical,turbulence},
month = {apr},
number = {1},
pages = {53--110},
title = {{A new class of accurate, mesh-free hydrodynamic simulation methods}},
url = {http://adsabs.harvard.edu/abs/2014arXiv1409.7395H},
volume = {450},
year = {2015}
}
@inproceedings{Yu1998,
abstract = {Bitmap indexing, though effective for low cardinality attributes, can be rather costly in storage overhead for high cardinality attributes. Range-based bitmap (RBM) indexing can be used to reduce this storage overhead. The attribute values are partitioned into ranges and a bitmap vector is used to represent a range. With RBM, however, the number of records assigned to different ranges can be highly uneven, resulting in non-uniform search times for different queries. We present and evaluate a dynamic bucket expansion and contraction (DBEC) approach to simultaneously constructing range-based bitmap indexes for multiple high-cardinality attributes. Simulations are conducted to evaluate this DBEC approach. Both synthetic and real data are used in the simulations. The results show that (1) with highly skewed data, DBEC performs quite well compared with a simple approach and (2) DBEC compares favorably with the optimal approach},
author = {Yu, P.S.},
booktitle = {Proceedings. Twenty-Second Annu. Int. Comput. Softw. Appl. Conf. (Compsac '98) (Cat. No.98CB 36241)},
doi = {10.1109/CMPSAC.1998.716637},
isbn = {0-8186-8585-9},
issn = {0730-3157},
keywords = {Data analysis,Decision support systems,Decoding,Indexing,Multidimensional systems,RBM,Simultaneous localization and mapping,attribute values,bitmap indexing,data analysis,decision support systems,multidimensional data analysis,multidimensional index structure,non-uniform search times,range-based bitmap,storage management},
pages = {61--66},
publisher = {IEEE Comput. Soc},
shorttitle = {Computer Software and Applications Conference, 199},
title = {{Range-based bitmap indexing for high cardinality attributes with skew}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=716637},
year = {1998}
}
@inproceedings{Wu2003,
author = {Wu, Kesheng and Koegler, W. and Chen, J. and Shoshani, A.},
booktitle = {15th Int. Conf. Sci. Stat. Database Manag. 2003.},
doi = {10.1109/SSDM.2003.1214955},
isbn = {0-7695-1964-4},
issn = {1099-3371},
keywords = {Analytical models,Combustion,Data visualization,Energy management,Indexing,Laboratories,Octree,Performance analysis,Spatiotemporal phenomena,Temperature,Testing,bitmap compression,bitmap index,combustion simulation,content-based retrieval,data analysis,data exploration,data tracking,database indexing,image coding,interactive exploration,isocontouring algorithm,spatial data structures,spatio-temporal dataset,temporal databases,tree data structures,tree searching,tree-based indexing,visual databases},
language = {English},
pages = {65--74},
publisher = {IEEE Comput. Soc},
title = {{Using bitmap index for interactive exploration of large datasets}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1214955},
year = {2003}
}
@article{Juric2015a,
abstract = {The Large Synoptic Survey Telescope (LSST) is a large-aperture, wide-field, ground-based survey system that will image the sky in six optical bands from 320 to 1050 nm, uniformly covering approximately {\$}18,000{\$}deg{\$}{\^{}}2{\$} of the sky over 800 times. The LSST is currently under construction on Cerro Pach$\backslash$'on in Chile, and expected to enter operations in 2022. Once operational, the LSST will explore a wide range of astrophysical questions, from discovering "killer" asteroids to examining the nature of Dark Energy. The LSST will generate on average 15 TB of data per night, and will require a comprehensive Data Management system to reduce the raw data to scientifically useful catalogs and images with minimum human intervention. These reductions will result in a real-time alert stream, and eleven data releases over the 10-year duration of LSST operations. To enable this processing, the LSST project is developing a new, general-purpose, high-performance, scalable, well documented, open source data processing software stack for O/IR surveys. Prototypes of this stack are already capable of processing data from existing cameras (e.g., SDSS, DECam, MegaCam), and form the basis of the Hyper-Suprime Cam (HSC) Survey data reduction pipeline.},
author = {Juri{\'{c}},  Mario and Kantor,  Jeffrey and Lim,  K-T and Lupton,  Robert H. and Dubois-Felsmann,  Gregory and Jenness,  Tim and Axelrod,  Tim S. and Aleksi{\'{c}},  Jovan and Allsman,  Roberta A. and AlSayyad,  Yusra and Alt,  Jason and Armstrong,  Robert and Basney,  Jim and Becker,  Andrew C. and Becla,  Jacek and Bickerton,  Steven J. and Biswas,  Rahul and Bosch,  James and Boutigny,  Dominique and {Carrasco Kind},  Matias and Ciardi,  David R. and Connolly,  Andrew J. and Daniel,  Scott F. and Daues,  Gregory E. and Economou,  Frossie and Chiang,  Hsin-Fang and Fausti,  Angelo and Fisher-Levine,  Merlin and Freemon,  D. Michael and Gee,  Perry and Gris,  Philippe and Hernandez,  Fabio and Hoblitt,  Joshua and Ivezi{\'{c}},  {\v{Z}}eljko and Jammes,  Fabrice and Jevremovi{\'{c}},  Darko and Jones,  R. Lynne and {Bryce Kalmbach},  J. and Kasliwal,  Vishal P. and Krughoff,  K. Simon and Lang,  Dustin and Lurie,  John and Lust,  Nate B. and Mullally,  Fergal and MacArthur,  Lauren A. and Melchior,  Peter and Moeyens,  Joachim and Nidever,  David L. and Owen,  Russell and Parejko,  John K and Peterson,  J. Matt and Petravick,  Donald and Pietrowicz,  Stephen R. and Price,  Paul A. and Reiss,  David J. and Shaw,  Richard A. and Sick,  Jonathan and Slater,  Colin T. and Strauss,  Michael A. and Sullivan,  Ian S. and Swinbank,  John D. and {Van Dyk},  Schuyler and Vuj{\v{c}}i{\'{c}},  Veljko and Withers,  Alexander and Yoachim,  Peter and {LSST Project},  for the},
journal = {eprint arXiv:1512.07914},
keywords = {Astrophysics - Instrumentation and Methods for Ast},
title = {{The LSST Data Management System}},
url = {http://adsabs.harvard.edu/abs/2015arXiv151207914J},
year = {2015}
}
@article{Springel2001,
abstract = {We describe the newly written code GADGET which is suitable both for cosmological simulations of structure formation and for the simulation of interacting galaxies. GADGET evolves self-gravitating collisionless fluids with the traditional N-body approach, and a collisional gas by smoothed particle hydrodynamics. Along with the serial version of the code, we discuss a parallel version that has been designed to run on massively parallel supercomputers with distributed memory. While both versions use a tree algorithm to compute gravitational forces, the serial version of GADGET can optionally employ the special-purpose hardware GRAPE instead of the tree. Periodic boundary conditions are supported by means of an Ewald summation technique. The code uses individual and adaptive timesteps for all particles, and it combines this with a scheme for dynamic tree updates. Due to its Lagrangian nature, GADGET thus allows a very large dynamic range to be bridged, both in space and time. So far, GADGET has been successfully used to run simulations with up to 7.5×107 particles, including cosmological studies of large-scale structure formation, high-resolution simulations of the formation of clusters of galaxies, as well as workstation-sized problems of interacting galaxies. In this study, we detail the numerical algorithms employed, and show various tests of the code. We publicly release both the serial and the massively parallel version of the code.},
author = {Springel, Volker and Yoshida, Naoki and White, Simon D. M.},
doi = {10.1016/S1384-1076(01)00042-2},
issn = {13841076},
journal = {New Astron.},
keywords = {codes: GADGET},
mendeley-tags = {codes: GADGET},
month = {apr},
number = {2},
pages = {79--117},
title = {{GADGET: a code for collisionless and gasdynamical cosmological simulations}},
url = {http://adsabs.harvard.edu/abs/2001NewA....6...79S},
volume = {6},
year = {2001}
}
@article{Gargantini1982,
author = {Gargantini, Irene},
doi = {10.1145/358728.358741},
issn = {00010782},
journal = {Commun. ACM},
month = {dec},
number = {12},
pages = {905--910},
title = {{An effective way to represent quadtrees}},
url = {http://portal.acm.org/citation.cfm?doid=358728.358741},
volume = {25},
year = {1982}
}
@inproceedings{Orenstein1984,
address = {New York, New York, USA},
author = {Orenstein, J. A. and Merrett, T. H.},
booktitle = {Proc. 3rd ACM SIGACT-SIGMOD Symp. Princ. database Syst. - Pod. '84},
doi = {10.1145/588011.588037},
file = {:Users/langmm/Library/Application Support/Mendeley Desktop/Downloaded/Orenstein, Merrett - 1984 - A class of data structures for associative searching.pdf:pdf},
isbn = {0897911288},
month = {apr},
pages = {181},
publisher = {ACM Press},
title = {{A class of data structures for associative searching}},
url = {http://dl.acm.org/citation.cfm?id=588011.588037},
year = {1984}
}
@article{Hjaltason2002,
author = {Hjaltason, Gisli R. and Samet, Hanan},
doi = {10.1007/s00778-002-0067-8},
issn = {10668888},
journal = {VLDB J. Int. J. Very Large Data Bases},
month = {oct},
number = {2},
pages = {109--137},
title = {{Speeding up construction of PMR quadtree-based spatial indexes}},
url = {http://link.springer.com/10.1007/s00778-002-0067-8},
volume = {11},
year = {2002}
}
@techreport{Morton1996,
address = {Ottawa, Ontario},
author = {Morton, G. M.},
institution = {IBM Ltd.},
title = {{A computer oriented geodetic data base and a new technique in file sequencing}},
year = {1996}
}
@article{Chan1998,
author = {Chan, Chee-Yong and Ioannidis, Yannis E.},
doi = {10.1145/276305.276336},
file = {:Users/langmm/Library/Application Support/Mendeley Desktop/Downloaded/Chan, Ioannidis - 1998 - Bitmap index design and evaluation.pdf:pdf},
isbn = {0-89791-995-5},
issn = {01635808},
journal = {ACM SIGMOD Rec.},
month = {jun},
number = {2},
pages = {355--366},
publisher = {ACM},
title = {{Bitmap index design and evaluation}},
url = {http://dl.acm.org/citation.cfm?id=276305.276336},
volume = {27},
year = {1998}
}
@article{Chan1999,
author = {Chan, Chee-Yong and Ioannidis, Yannis E.},
doi = {10.1145/304181.304201},
file = {:Users/langmm/Library/Application Support/Mendeley Desktop/Downloaded/Chan, Ioannidis - 1999 - An efficient bitmap encoding scheme for selection queries.pdf:pdf},
isbn = {1-58113-084-8},
issn = {01635808},
journal = {ACM SIGMOD Rec.},
month = {jun},
number = {2},
pages = {215--226},
publisher = {ACM},
title = {{An efficient bitmap encoding scheme for selection queries}},
url = {http://dl.acm.org/citation.cfm?id=304181.304201},
volume = {28},
year = {1999}
}
@incollection{Stockinger2000,
address = {Berlin, Heidelberg},
author = {Stockinger, Kurt and Duellmann, Dirk and Hoschek, Wolfgang and Schikuta, Erich},
booktitle = {Database Expert Syst. Appl. 11th Int. Conf. DEXA 2000 London, UK, Sept. 4--8, 2000 Proc.},
chapter = {Improving },
doi = {10.1007/3-540-44469-6},
editor = {Ibrahim, Mohamed and K{\"{u}}ng, Josef and Revell, Norman},
isbn = {978-3-540-67978-3},
month = {jun},
pages = {835--845},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Improving the Performance of High-Energy Physics Analysis through Bitmap Indices}},
url = {http://link.springer.com/10.1007/3-540-44469-6},
volume = {1873},
year = {2000}
}
@article{Bern1999,
author = {Bern, Marshall and Eppstein, David and Teng, Shang-Hua},
doi = {10.1142/S0218195999000303},
issn = {0218-1959},
journal = {Int. J. Comput. Geom. Appl.},
month = {dec},
number = {06},
pages = {517--532},
title = {{PARALLEL CONSTRUCTION OF QUADTREES AND QUALITY TRIANGULATIONS}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218195999000303},
volume = {09},
year = {1999}
}
